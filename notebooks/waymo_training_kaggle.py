# %% [markdown]
# # üöó TrafficGamer - Waymo Motion Dataset Training on Kaggle
#
# **Complete notebook for training multi-agent RL policies on Waymo Open Motion Dataset v1.2**
#
# This notebook:
# - Loads Waymo Motion Dataset v1.2 via Waymax (streaming from GCS)
# - Uses QCNet for trajectory prediction baseline
# - Trains TrafficGamer / MAPPO / CCE-MAPPO policies
# - Visualizes scenarios and training progress
#
# **Repository:** https://github.com/PhamPhuHoa-23/EvolutionaryTest

# %% [markdown]
# ## 1. Install Dependencies

# %%
# Core dependencies
from utils.rollout import PPO_process_batch
from utils.utils import (
    seed_everything,
    dict_mean,
    reward_function,
    cost_function,
    get_auto_pred,
    get_v_transform_mat,
    generate_tmp_gif_path,
)
from transforms import TargetBuilder
from predictors.autoval import AutoQCNet
from datasets.waymo_dataset import WaymoDataset
from algorithm.TrafficGamer import TrafficGamer
from algorithm.constrainted_cce_mappo import Constrainted_CCE_MAPPO
from algorithm.cce_mappo import CCE_MAPPO
from algorithm.mappo import MAPPO
from torch_geometric.loader import DataLoader
from torch_geometric.data import Batch
from torch.distributions import Laplace
import torch.nn.functional as F
import torch.nn as nn
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import pandas as pd
import pickle
import yaml
from pathlib import Path
import warnings
import sys
import os
import numpy as np
import tensorflow as tf
import torch
!pip install - q torch torchvision torchaudio
!pip install - q pytorch-lightning == 2.0.0
!pip install - q torch-geometric
!pip install - q tensorflow  # For Waymo dataset loading
!pip install - q av  # Video encoding
!pip install - q av2  # Argoverse 2 API (needed by source imports)

# %%
# Waymax - JAX-based simulator for Waymo
!pip install - -upgrade pip
!pip install git+https: // github.com/waymo-research/waymax.git@main  # egg=waymo-waymax

# %%

# Check versions
print(f"PyTorch version: {torch.__version__}")
print(f"TensorFlow version: {tf.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    cuda_version = torch.version.cuda.replace('.', '')
    print(f"CUDA version: {cuda_version}")
else:
    cuda_version = 'cpu'
    print("Running on CPU")

# %%
# Install PyTorch Geometric dependencies
if torch.cuda.is_available():
    !pip install torch-scatter torch-sparse torch-cluster - f https: // data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cu{cuda_version[:3]}.html
else:
    !pip install torch-scatter torch-sparse torch-cluster - f https: // data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cpu.html

# %% [markdown]
# ## 2. Setup Environment

# %%

warnings.filterwarnings('ignore')
# Disable TF GPU (use PyTorch for training)
tf.config.set_visible_devices([], 'GPU')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"‚úÖ Using device: {device}")

# %% [markdown]
# ## 3. Clone TrafficGamer Repository

# %%
# Clone repo
REPO_DIR = Path("EvolutionaryTest")

if not REPO_DIR.exists():
    print("üì• Cloning EvolutionaryTest repository...")
    !git clone https: // github.com/PhamPhuHoa-23/EvolutionaryTest.git
    print("‚úÖ Cloned successfully")
else:
    print("‚úÖ Repository already exists")
    # Pull latest changes
    !cd EvolutionaryTest & & git pull

# Add to path and change directory
sys.path.insert(0, str(REPO_DIR.absolute()))
os.chdir(REPO_DIR)
print(f"üìÅ Working directory: {os.getcwd()}")

# %%
# Install requirements
!pip install - q - r requirements.txt
!pip install - q neptune wandb

# %% [markdown]
# ## 4. Import TrafficGamer Components

# %%

# TrafficGamer imports

print("‚úÖ TrafficGamer components imported successfully")

# %% [markdown]
# ## 5. Configuration
#
# **üîß EDIT HERE - Configure paths and parameters:**

# %%
# ============================================
# üîß CONFIGURATION - EDIT HERE
# ============================================

CONFIG = {
    # Kaggle paths
    # Waymo dataset root on Kaggle
    'waymo_root': '/kaggle/input/waymo-motion-dataset-v120',
    'processed_dir': '/kaggle/working/processed',  # Processed data directory
    # Pre-trained QCNet checkpoint
    'checkpoint_path': '/kaggle/input/qcnet-waymo/epoch=20-step=79905.ckpt',

    # Data split
    'split': 'val',  # 'train', 'val', or 'test'

    # Dataset settings (Waymo: 11 history + 80 future @ 10Hz)
    'num_historical_steps': 11,
    'num_future_steps': 80,

    # Training settings
    'seed': 42,
    'batch_size': 32,
    'num_workers': 4,
    'episodes': 300,

    # RL algorithm: 'TrafficGamer', 'MAPPO', 'CCE_MAPPO', 'Constrainted_CCE_MAPPO'
    'rl_algorithm': 'TrafficGamer',
    'rl_config_file': 'TrafficGamer.yaml',

    # Scenario settings - predefined scenarios from paper
    'scenario_id': 1,  # 1-6

    # Training hyperparameters (from config file, can override)
    'offset': 5,  # Action step interval
    'hidden_dim': 128,

    # Safety constraints
    'distance_limit': 5.0,  # Collision threshold (meters)
    'cost_quantile': 48,
    'penalty_initial_value': 1.0,

    # Magnet reward (exploration bonus)
    'magnet': True,
    'eta_coef1': 0.05,
    'eta_coef2': 0.05,

    # Logging
    'track': False,  # Enable wandb tracking
    'save_checkpoint': True,
}

# Set seeds
seed_everything(CONFIG['seed'])

print("‚úÖ Configuration loaded")
for key, value in CONFIG.items():
    print(f"   {key}: {value}")

# %% [markdown]
# ## 6. Authenticate with Google Cloud (for Waymax streaming)
#
# **Choose ONE method:**
# - **Kaggle**: Use Service Account Key uploaded as dataset
# - **Colab**: Use `auth.authenticate_user()`

# %%
print("üîë Authenticating with Google Cloud...")

# Method 1: Service Account Key (Kaggle)
# Upload your GCS credentials as Kaggle dataset
service_key_path = '/kaggle/input/gcs-credentials/auth.json'

if os.path.exists(service_key_path):
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = service_key_path
    print(f"‚úÖ Authenticated via service account: {service_key_path}")
else:
    print("‚ö†Ô∏è  No service account found. Using default credentials...")
    print("   For Colab, run: from google.colab import auth; auth.authenticate_user()")

# %% [markdown]
# ## 7. Load Pre-trained QCNet Model

# %%
print("üì¶ Loading pre-trained QCNet model...")

# Check if checkpoint exists
if not os.path.exists(CONFIG['checkpoint_path']):
    print(f"‚ö†Ô∏è  Checkpoint not found at {CONFIG['checkpoint_path']}")
    print("   Please upload QCNet checkpoint as Kaggle dataset")
    print("   Or download from: https://github.com/ZikangZhou/QCNet")
    raise FileNotFoundError(
        f"Checkpoint not found: {CONFIG['checkpoint_path']}")

# Load model
model = AutoQCNet.load_from_checkpoint(
    checkpoint_path=CONFIG['checkpoint_path'])
model = model.to(device)
model.eval()

# Freeze encoder and decoder (only train RL policy)
for param in model.encoder.parameters():
    param.requires_grad = False
for param in model.decoder.parameters():
    param.requires_grad = False

print(f"‚úÖ QCNet loaded successfully")
print(f"   Dataset: {model.dataset}")
print(f"   Num modes: {model.num_modes}")
print(f"   Historical steps: {model.num_historical_steps}")
print(f"   Future steps: {model.num_future_steps}")
print(f"   Output dim: {model.output_dim}")

# %% [markdown]
# ## 8. Load Waymo Dataset
#
# **Option A: Local processed data (faster)**
# **Option B: Waymax streaming (no download needed)**

# %%
# Predefined scenarios from TrafficGamer paper
SCENARIOS = {
    1: {
        'scene_id': '9c6eb32bcc69d42e',
        'agent_ids': [0, 1, 2, 3, 137, 42, 52],
        'description': 'Multi-vehicle intersection'
    },
    2: {
        'scene_id': '2f1be7eedc2c7333',
        'agent_ids': [983, 994, 987, 980, 1007, 1475],
        'description': 'Highway merging'
    },
    3: {
        'scene_id': 'ab68832bf7312ab3',
        'agent_ids': [387, 354, 352, 384, 385, 386],
        'description': 'Roundabout'
    },
    4: {
        'scene_id': '63bcffc229444c56',
        'agent_ids': [230, 758, 713, 714, 715],
        'description': 'T-junction'
    },
    5: {
        'scene_id': 'caea26e357c20bfc',
        'agent_ids': [2159, 2160, 2161, 2162, 2163, 2164],
        'description': 'Parking lot'
    },
    6: {
        'scene_id': '5d0c97b991689cde',
        'agent_ids': [891, 885, 1188, 887, 888, 890],
        'description': 'Urban street'
    },
}

scenario_info = SCENARIOS[CONFIG['scenario_id']]
print(
    f"üìã Selected Scenario {CONFIG['scenario_id']}: {scenario_info['description']}")
print(f"   Scene ID: {scenario_info['scene_id']}")
print(f"   Agent IDs: {scenario_info['agent_ids']}")

# %% [markdown]
# ### Option A: Load from processed data (if available)

# %%
USE_LOCAL_DATA = True  # Set to False to use Waymax streaming

if USE_LOCAL_DATA:
    print("üìÇ Loading Waymo dataset from processed files...")

    try:
        val_dataset = WaymoDataset(
            root=CONFIG['waymo_root'],
            processed_dir=CONFIG['processed_dir'],
            split=CONFIG['split'],
            transform=TargetBuilder(
                model.num_historical_steps, model.num_future_steps),
        )

        # Find scenario index
        scene_id = scenario_info['scene_id']
        try:
            scenario_idx = val_dataset.processed_file_names.index(
                f'{scene_id}.pkl')
            print(f"‚úÖ Found scenario at index {scenario_idx}")
        except ValueError:
            print(f"‚ö†Ô∏è  Scenario {scene_id} not found in processed files")
            print(f"   Using first available scenario instead")
            scenario_idx = 0

        # Create dataloader for single scenario
        dataloader = DataLoader(
            val_dataset[[scenario_idx]],
            batch_size=1,
            shuffle=False,
            num_workers=CONFIG['num_workers'],
            pin_memory=True,
        )

        # Get data
        data = next(iter(dataloader))
        print(f"‚úÖ Dataset loaded successfully")
        print(f"   Num agents: {data['agent']['num_nodes']}")
        print(f"   Scenario ID: {data['scenario_id'][0]}")

    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to load local data: {e}")
        print("   Falling back to Waymax streaming...")
        USE_LOCAL_DATA = False

# %% [markdown]
# ### Option B: Load via Waymax streaming

# %%
if not USE_LOCAL_DATA:
    print("üìÇ Loading Waymo dataset via Waymax streaming...")

    import dataclasses
    from waymax import config as waymax_config
    from waymax import dataloader as waymax_dataloader
    from waymax import env, dynamics, datatypes
    from waymax import visualization as waymax_viz

    # Configure dataset
    if CONFIG['split'] == 'train':
        dataset_config = waymax_config.WOD_1_2_0_TRAINING
    elif CONFIG['split'] == 'val':
        dataset_config = waymax_config.WOD_1_2_0_VALIDATION
    else:
        dataset_config = waymax_config.WOD_1_2_0_TESTING

    # Fix path and configure
    dataset_config_fix = dataset_config.path.replace("///", "//")
    dataset_config = dataclasses.replace(
        dataset_config,
        max_num_objects=64,
        path=dataset_config_fix
    )

    # Create iterator
    waymo_iterator = waymax_dataloader.simulator_state_generator(
        dataset_config)

    print(f"‚úÖ Waymax iterator created for {CONFIG['split']} split")
    print(f"   Data streams from: {dataset_config.path}")

    # Get sample scenario
    sample_state = next(waymo_iterator)
    print(f"\nüìä Sample scenario loaded:")
    print(f"   Timesteps: {sample_state.timestep}")
    print(f"   Objects: {sample_state.num_objects}")

# %% [markdown]
# ## 9. Visualize Scenario

# %%
if USE_LOCAL_DATA and 'data' in dir():
    print("üé® Visualizing scenario...")

    fig, axes = plt.subplots(1, 2, figsize=(16, 8))

    # Plot agent positions at historical timestep
    ax = axes[0]
    positions = data['agent']['position'][:,
                                          model.num_historical_steps - 1, :2].cpu().numpy()
    valid = data['agent']['valid_mask'][:,
                                        model.num_historical_steps - 1].cpu().numpy()

    # Plot all agents
    ax.scatter(positions[valid, 0], positions[valid, 1],
               c='blue', s=50, alpha=0.5, label='Other agents')

    # Highlight controlled agents
    agent_ids = scenario_info['agent_ids']
    choose_agent = [i for i, j in enumerate(
        data['agent']['id'][0]) if j in agent_ids]

    for idx in choose_agent:
        ax.scatter(positions[idx, 0], positions[idx, 1],
                   c='red', s=100, marker='*', label=f'Agent {idx}')

    ax.set_xlabel('X (m)')
    ax.set_ylabel('Y (m)')
    ax.set_title(f'Scenario {CONFIG["scenario_id"]}: Agent Positions at t=0')
    ax.legend()
    ax.set_aspect('equal')
    ax.grid(True, alpha=0.3)

    # Plot trajectories
    ax = axes[1]
    for idx in choose_agent[:4]:  # Show first 4 controlled agents
        traj = data['agent']['position'][idx, :, :2].cpu().numpy()
        valid_traj = data['agent']['valid_mask'][idx, :].cpu().numpy()

        # Historical trajectory
        ax.plot(traj[:model.num_historical_steps, 0], traj[:model.num_historical_steps, 1],
                'b-', linewidth=2, alpha=0.7)
        # Future trajectory (ground truth)
        ax.plot(traj[model.num_historical_steps:, 0], traj[model.num_historical_steps:, 1],
                'g--', linewidth=1, alpha=0.5)

        ax.scatter(traj[model.num_historical_steps - 1, 0], traj[model.num_historical_steps - 1, 1],
                   c='red', s=100, marker='o')

    ax.set_xlabel('X (m)')
    ax.set_ylabel('Y (m)')
    ax.set_title('Agent Trajectories (Blue: History, Green: Future GT)')
    ax.set_aspect('equal')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('scenario_visualization.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("‚úÖ Visualization saved to 'scenario_visualization.png'")

elif not USE_LOCAL_DATA and 'sample_state' in dir():
    print("üé® Visualizing Waymax scenario...")

    try:
        img = waymax_viz.plot_simulator_state(sample_state, use_log_traj=True)
        plt.figure(figsize=(12, 10))
        plt.imshow(img)
        plt.axis('off')
        plt.title('Waymo Scenario Visualization',
                  fontsize=14, fontweight='bold')
        plt.savefig('waymax_scenario.png', dpi=150, bbox_inches='tight')
        plt.show()
        print("‚úÖ Waymax visualization saved")
    except Exception as e:
        print(f"‚ö†Ô∏è  Visualization failed: {e}")

# %% [markdown]
# ## 10. Load RL Configuration

# %%
print("üìã Loading RL configuration...")

# Load config file
rl_config_path = Path('configs') / CONFIG['rl_config_file']
with open(rl_config_path, 'r') as f:
    rl_config = yaml.safe_load(f)

# Override with user settings
rl_config['batch_size'] = CONFIG['batch_size']
rl_config['episodes'] = CONFIG['episodes']
rl_config['eta_coef1'] = CONFIG['eta_coef1']
rl_config['eta_coef2'] = CONFIG['eta_coef2']
rl_config['is_magnet'] = CONFIG['magnet']
rl_config['penalty_initial_value'] = CONFIG['penalty_initial_value']
rl_config['cost_quantile'] = CONFIG['cost_quantile']

print("‚úÖ RL Configuration:")
for key, value in rl_config.items():
    print(f"   {key}: {value}")

# %% [markdown]
# ## 11. Initialize RL Agents

# %%
print("ü§ñ Initializing RL agents...")

# Get controlled agent indices
if USE_LOCAL_DATA:
    agent_ids = scenario_info['agent_ids']
    choose_agent = [i for i, j in enumerate(
        data['agent']['id'][0]) if j in agent_ids]
else:
    # For Waymax, use first N valid agents
    choose_agent = list(range(min(6, int(sample_state.num_objects))))

agent_num = len(choose_agent)
rl_config['agent_number'] = agent_num

print(f"   Controlling {agent_num} agents: {choose_agent}")

# State dimension = num_modes * hidden_dim
STATE_DIM = model.num_modes * rl_config['hidden_dim']

# Initialize agents based on algorithm
if CONFIG['rl_algorithm'] == 'TrafficGamer':
    agents = [
        TrafficGamer(
            state_dim=STATE_DIM,
            agent_number=agent_num,
            config=rl_config,
            device=device,
        )
        for _ in range(agent_num)
    ]
    print(f"‚úÖ Initialized {agent_num} TrafficGamer agents")

elif CONFIG['rl_algorithm'] == 'Constrainted_CCE_MAPPO':
    agents = [
        Constrainted_CCE_MAPPO(
            state_dim=STATE_DIM,
            agent_number=agent_num,
            config=rl_config,
            device=device,
        )
        for _ in range(agent_num)
    ]
    print(f"‚úÖ Initialized {agent_num} Constrainted CCE-MAPPO agents")

elif CONFIG['rl_algorithm'] == 'CCE_MAPPO':
    agents = [
        CCE_MAPPO(
            state_dim=STATE_DIM,
            agent_number=agent_num,
            config=rl_config,
            device=device,
        )
        for _ in range(agent_num)
    ]
    print(f"‚úÖ Initialized {agent_num} CCE-MAPPO agents")

else:  # MAPPO
    agents = [
        MAPPO(
            state_dim=STATE_DIM,
            agent_number=agent_num,
            config=rl_config,
            device=device,
        )
        for _ in range(agent_num)
    ]
    print(f"‚úÖ Initialized {agent_num} MAPPO agents")

# %% [markdown]
# ## 12. Load Map Features (for reward computation)

# %%
print("üó∫Ô∏è Loading map features...")

# Try to load ScenarioNet map features if available
scenarionet_path = Path('scenarionet/datasets/way_convert')
scenarionet_pkl = f'sd_waymo_v1.2_{scenario_info["scene_id"]}.pkl'

scenario_static_map = None

if scenarionet_path.exists():
    try:
        df_mapping = pd.read_pickle(scenarionet_path / 'dataset_mapping.pkl')
        pkl_loc = df_mapping.get(scenarionet_pkl, '')

        if pkl_loc:
            df = pd.read_pickle(scenarionet_path / pkl_loc / scenarionet_pkl)
            scenario_static_map = df['map_features']
            print(f"‚úÖ Loaded map features from ScenarioNet")
        else:
            print("‚ö†Ô∏è  Map features not found in ScenarioNet")
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to load ScenarioNet map: {e}")

if scenario_static_map is None:
    print("   Using simplified map from dataset")
    # Will use data['map'] if available, or skip map-based rewards

# %% [markdown]
# ## 13. Training Loop

# %%
print("üöÄ Starting training...")
print(f"   Algorithm: {CONFIG['rl_algorithm']}")
print(f"   Episodes: {rl_config['episodes']}")
print(f"   Batch size: {rl_config['batch_size']}")
print(f"   Offset: {rl_config['offset']}")

# Training metrics
training_metrics = {
    'episode_rewards': [],
    'episode_costs': [],
    'agent_rewards': [[] for _ in range(agent_num)],
    'actor_losses': [],
    'critic_losses': [],
}

offset = rl_config['offset']

# Namespace for args (compatibility with rollout functions)


class Args:
    def __init__(self, config):
        self.scenario = config['scenario_id']
        self.distance_limit = config['distance_limit']
        self.track = config['track']
        self.confined_action = False
        self.magnet = config['magnet']
        self.eta_coef1 = config['eta_coef1']
        self.eta_coef2 = config['eta_coef2']
        self.threeD = False
        self.save = config['save_checkpoint']


args = Args(CONFIG)

# %%
# Main training loop
for episode in tqdm(range(rl_config['episodes']), desc="Training"):

    # Initialize transition storage
    transition_list = [
        {
            "observations": [[] for _ in range(agent_num)],
            "actions": [[] for _ in range(agent_num)],
            "next_observations": [[] for _ in range(agent_num)],
            "rewards": [[] for _ in range(agent_num)],
            "magnet": [[] for _ in range(agent_num)],
            "costs": [[] for _ in range(agent_num)],
            "dones": [],
        }
        for _ in range(rl_config['batch_size'])
    ]

    episode_total_reward = 0.0
    episode_total_cost = 0.0

    # Batch processing
    with torch.no_grad():
        for batch in range(rl_config['batch_size']):
            # Render only occasionally for logging
            render = (batch == 0) and ((episode + 1) % 10 == 0) and args.track

            # Process batch using PPO rollout
            try:
                PPO_process_batch(
                    args=args,
                    batch=batch,
                    new_input_data=data.clone(),
                    model=model,
                    agents=agents,
                    choose_agent=choose_agent,
                    offset=offset,
                    scenario_static_map=scenario_static_map,
                    scenario_num=CONFIG['scenario_id'],
                    transition_list=transition_list,
                    render=render,
                    agent_num=agent_num,
                    dataset_type='waymo'
                )
            except Exception as e:
                if episode == 0:
                    print(f"‚ö†Ô∏è  Rollout error (batch {batch}): {e}")
                continue

    # Update agents
    agent_logs = []
    for i, agent in enumerate(agents):
        try:
            logs = agent.update(transition_list, i)
            agent_logs.append(logs)

            # Aggregate rewards
            total_reward = sum(
                r.item() if hasattr(r, 'item') else r
                for t in transition_list
                for r in t['rewards'][i]
            )
            total_cost = sum(
                c.item() if hasattr(c, 'item') else c
                for t in transition_list
                for c in t['costs'][i]
            )

            training_metrics['agent_rewards'][i].append(total_reward)
            episode_total_reward += total_reward
            episode_total_cost += total_cost

        except Exception as e:
            if episode == 0:
                print(f"‚ö†Ô∏è  Update error (agent {i}): {e}")

    # Record metrics
    training_metrics['episode_rewards'].append(
        episode_total_reward / agent_num)
    training_metrics['episode_costs'].append(episode_total_cost / agent_num)

    # Log progress
    if (episode + 1) % 10 == 0:
        avg_reward = np.mean(training_metrics['episode_rewards'][-10:])
        avg_cost = np.mean(training_metrics['episode_costs'][-10:])
        print(f"\nüìä Episode {episode + 1}/{rl_config['episodes']}")
        print(f"   Avg Reward: {avg_reward:.4f}")
        print(f"   Avg Cost: {avg_cost:.4f}")

print("\n‚úÖ Training completed!")

# %% [markdown]
# ## 14. Visualize Training Progress

# %%
print("üìà Visualizing training progress...")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Episode rewards
ax = axes[0, 0]
ax.plot(training_metrics['episode_rewards'], alpha=0.7, label='Episode Reward')
# Moving average
window = min(20, len(training_metrics['episode_rewards']))
if window > 1:
    rewards_ma = pd.Series(training_metrics['episode_rewards']).rolling(
        window=window).mean()
    ax.plot(rewards_ma, color='red', linewidth=2, label=f'{window}-ep MA')
ax.set_title('Episode Rewards', fontsize=12, fontweight='bold')
ax.set_xlabel('Episode')
ax.set_ylabel('Total Reward')
ax.legend()
ax.grid(True, alpha=0.3)

# Episode costs
ax = axes[0, 1]
ax.plot(training_metrics['episode_costs'], alpha=0.7, color='orange')
ax.set_title('Episode Safety Costs', fontsize=12, fontweight='bold')
ax.set_xlabel('Episode')
ax.set_ylabel('Total Cost')
ax.grid(True, alpha=0.3)

# Per-agent rewards
ax = axes[1, 0]
for i in range(min(4, agent_num)):
    ax.plot(training_metrics['agent_rewards']
            [i], alpha=0.7, label=f'Agent {i}')
ax.set_title('Per-Agent Rewards', fontsize=12, fontweight='bold')
ax.set_xlabel('Episode')
ax.set_ylabel('Reward')
ax.legend()
ax.grid(True, alpha=0.3)

# Summary statistics
ax = axes[1, 1]
ax.axis('off')
summary_text = f"""
Training Summary
================
Algorithm: {CONFIG['rl_algorithm']}
Total Episodes: {len(training_metrics['episode_rewards'])}
Controlled Agents: {agent_num}
Scenario: {CONFIG['scenario_id']}

Final Metrics (last 50 episodes):
- Avg Reward: {np.mean(training_metrics['episode_rewards'][-50:]):.4f}
- Avg Cost: {np.mean(training_metrics['episode_costs'][-50:]):.4f}

Best Episode Reward: {max(training_metrics['episode_rewards']) if training_metrics['episode_rewards'] else 0:.4f}
"""
ax.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',
        verticalalignment='center', transform=ax.transAxes)

plt.tight_layout()
plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úÖ Training visualization saved to 'training_progress.png'")

# %% [markdown]
# ## 15. Save Trained Models

# %%
if CONFIG['save_checkpoint']:
    print("üíæ Saving trained models...")

    save_dir = Path('checkpoints')
    save_dir.mkdir(exist_ok=True)

    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')

    for i, agent in enumerate(agents):
        checkpoint = {
            'agent_index': i,
            'state_dim': STATE_DIM,
            'agent_number': agent_num,
            'config': rl_config,
            'policy_state_dict': agent.pi.state_dict(),
            'value_state_dict': agent.value.state_dict(),
        }

        save_path = save_dir / \
            f'{CONFIG["rl_algorithm"]}_scenario{CONFIG["scenario_id"]}_agent{i}_{timestamp}.pt'
        torch.save(checkpoint, save_path)
        print(f"   ‚úÖ Saved Agent {i} to {save_path}")

    # Save training metrics
    metrics_path = save_dir / \
        f'metrics_scenario{CONFIG["scenario_id"]}_{timestamp}.pkl'
    with open(metrics_path, 'wb') as f:
        pickle.dump({
            'training': training_metrics,
            'config': CONFIG,
            'rl_config': rl_config,
        }, f)
    print(f"   ‚úÖ Saved metrics to {metrics_path}")

# %% [markdown]
# ## 16. Evaluation

# %%
print("üîç Evaluating trained policy...")

NUM_EVAL_EPISODES = 20
eval_metrics = {
    'rewards': [],
    'costs': [],
}

for eval_ep in tqdm(range(NUM_EVAL_EPISODES), desc="Evaluating"):
    eval_transition = [
        {
            "observations": [[] for _ in range(agent_num)],
            "actions": [[] for _ in range(agent_num)],
            "next_observations": [[] for _ in range(agent_num)],
            "rewards": [[] for _ in range(agent_num)],
            "magnet": [[] for _ in range(agent_num)],
            "costs": [[] for _ in range(agent_num)],
            "dones": [],
        }
    ]

    with torch.no_grad():
        try:
            PPO_process_batch(
                args=args,
                batch=0,
                new_input_data=data.clone(),
                model=model,
                agents=agents,
                choose_agent=choose_agent,
                offset=offset,
                scenario_static_map=scenario_static_map,
                scenario_num=CONFIG['scenario_id'],
                transition_list=eval_transition,
                render=False,
                agent_num=agent_num,
                dataset_type='waymo'
            )

            # Compute total reward/cost for this episode
            ep_reward = sum(
                r.item() if hasattr(r, 'item') else r
                for i in range(agent_num)
                for r in eval_transition[0]['rewards'][i]
            ) / agent_num

            ep_cost = sum(
                c.item() if hasattr(c, 'item') else c
                for i in range(agent_num)
                for c in eval_transition[0]['costs'][i]
            ) / agent_num

            eval_metrics['rewards'].append(ep_reward)
            eval_metrics['costs'].append(ep_cost)

        except Exception as e:
            pass

# Print evaluation results
print("\n" + "=" * 50)
print("üìä EVALUATION RESULTS")
print("=" * 50)
if eval_metrics['rewards']:
    print(
        f"Average Reward: {np.mean(eval_metrics['rewards']):.4f} ¬± {np.std(eval_metrics['rewards']):.4f}")
    print(
        f"Average Cost: {np.mean(eval_metrics['costs']):.4f} ¬± {np.std(eval_metrics['costs']):.4f}")
else:
    print("‚ö†Ô∏è  No valid evaluation episodes")
print("=" * 50)

# %% [markdown]
# ## 17. Summary
#
# **What we accomplished:**
# 1. ‚úÖ Loaded Waymo Motion Dataset v1.2
# 2. ‚úÖ Used pre-trained QCNet for trajectory prediction
# 3. ‚úÖ Trained multi-agent RL policy (TrafficGamer/MAPPO/CCE-MAPPO)
# 4. ‚úÖ Computed rewards based on:
#    - Goal reaching (distance to destination)
#    - Collision avoidance
#    - Road boundary constraints
#    - Speed limit compliance
# 5. ‚úÖ Visualized training progress
# 6. ‚úÖ Saved trained models
# 7. ‚úÖ Evaluated on validation scenarios
#
# **Repository:** https://github.com/PhamPhuHoa-23/EvolutionaryTest
