# %% [markdown]
# # ðŸš— TrafficGamer - Optimized Waymax Training (v6 - GPU Optimized)
#
# **Train multi-agent RL policies with better GPU utilization**
#
# **Optimizations in v6:**
# - âœ… Batched scenario loading with prefetch
# - âœ… JIT-compiled simulation functions
# - âœ… Vectorized operations where possible
# - âœ… Larger batch sizes for GPU efficiency
# - âœ… Reduced CPU-GPU data transfers
#
# **Repository:** https://github.com/PhamPhuHoa-23/EvolutionaryTest

# %% [markdown]
# ## 1. Install Dependencies

# %%
# Core dependencies
!pip install -q torch torchvision torchaudio
!pip install -q pytorch-lightning==2.0.0
!pip install -q torch-geometric

# %%
# Waymax - JAX-based simulator
!pip install --upgrade pip
!pip install git+https://github.com/waymo-research/waymax.git@main

# %%
!pip install -q tensorflow

# %%
import os
import sys
import warnings
from pathlib import Path
import pickle
import dataclasses
import functools
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Laplace, Normal
import numpy as np
import tensorflow as tf
import jax
import jax.numpy as jnp
from collections import deque
import threading
import queue

print(f"PyTorch version: {torch.__version__}")
print(f"JAX version: {jax.__version__}")
print(f"JAX devices: {jax.devices()}")
print(f"CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    cuda_version = torch.version.cuda.replace('.', '')
    print(f"CUDA version: {cuda_version}")
else:
    cuda_version = 'cpu'

# %%
# PyTorch Geometric dependencies
if torch.cuda.is_available():
    !pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cu{cuda_version[:3]}.html
else:
    !pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cpu.html

# %% [markdown]
# ## 2. Setup Environment

# %%
warnings.filterwarnings('ignore')
tf.config.set_visible_devices([], 'GPU')  # Disable TF GPU

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"âœ… PyTorch device: {device}")
print(f"âœ… JAX backend: {jax.default_backend()}")

# %% [markdown]
# ## 3. Authenticate with Google Cloud

# %%
print("ðŸ”‘ Authenticating with Google Cloud...")
service_key_path = '/kaggle/input/gcs-credentials/auth.json'

if os.path.exists(service_key_path):
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = service_key_path
    print(f"âœ… Authenticated via service account")
else:
    try:
        from google.colab import auth
        auth.authenticate_user()
        print("âœ… Authenticated via Colab")
    except ImportError:
        print("âš ï¸  No authentication found!")

# %% [markdown]
# ## 4. Clone Repository

# %%
REPO_DIR = Path("EvolutionaryTest")

if not REPO_DIR.exists():
    print("ðŸ“¥ Cloning repository...")
    !git clone https://github.com/PhamPhuHoa-23/EvolutionaryTest.git
else:
    print("âœ… Repository exists, pulling latest...")
    !cd EvolutionaryTest && git pull

sys.path.insert(0, str(REPO_DIR.absolute()))
os.chdir(REPO_DIR)
print(f"ðŸ“ Working directory: {os.getcwd()}")

# %%
!pip install -q -r requirements.txt

# %%
from waymax import config as waymax_config
from waymax import dataloader as waymax_dataloader
from waymax import env as waymax_env
from waymax import dynamics as waymax_dynamics
from waymax import datatypes
from waymax import visualization as waymax_viz

from algorithm.mappo import MAPPO
from algorithm.cce_mappo import CCE_MAPPO
from algorithm.constrainted_cce_mappo import Constrainted_CCE_MAPPO
from algorithm.TrafficGamer import TrafficGamer
from utils.utils import seed_everything

import pandas as pd

print("âœ… All imports successful")

# %% [markdown]
# ## 5. Optimized Configuration

# %%
CONFIG = {
    # Dataset
    'waymo_version': '1.2.0',
    'split': 'training',
    'max_num_objects': 32,

    # Observation - RICHER features
    # Per agent: self_state(7) + relative_to_others(3*3=9) + goal_info(4) + history_features(16) = 36
    'obs_dim': 36,  # Fixed calculation
    'history_length': 4,

    # Training - 10 HOUR RUN
    'seed': 42,
    'num_episodes': 6400,  # ~10 hours of training
    'steps_per_episode': 80,
    'batch_size': 64,

    # Algorithm
    'rl_algorithm': 'TrafficGamer',
    'num_controlled_agents': 4,
    'hidden_dim': 256,  # Increased for deeper network

    # RL hyperparameters
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'actor_lr': 3e-4,
    'critic_lr': 3e-4,
    'eps': 0.2,
    'entropy_coef': 0.005,
    'epochs': 15,  # More epochs per batch

    # Safety
    'distance_limit': 5.0,
    'cost_quantile': 48,
    'penalty_initial_value': 1.0,

    # Magnet
    'is_magnet': True,
    'eta_coef1': 0.05,

    # Optimization
    'prefetch_buffer_size': 16,
    'use_jit': True,
    'pin_memory': True,

    # Logging
    'log_interval': 10,  # Log less frequently
    'save_checkpoint': True,
}

seed_everything(CONFIG['seed'])
print("âœ… Configuration (10-Hour Training Run):")
for k, v in CONFIG.items():
    print(f"   {k}: {v}")

# %% [markdown]
# ## 6. Initialize Waymax with Prefetch

# %%
print("ðŸ“‚ Initializing Waymax data loader with prefetch...")

if CONFIG['waymo_version'] == '1.2.0':
    if CONFIG['split'] == 'training':
        dataset_config = waymax_config.WOD_1_2_0_TRAINING
    elif CONFIG['split'] == 'validation':
        dataset_config = waymax_config.WOD_1_2_0_VALIDATION
    else:
        dataset_config = waymax_config.WOD_1_2_0_TESTING
else:
    if CONFIG['split'] == 'training':
        dataset_config = waymax_config.WOD_1_1_0_TRAINING
    elif CONFIG['split'] == 'validation':
        dataset_config = waymax_config.WOD_1_1_0_VALIDATION
    else:
        dataset_config = waymax_config.WOD_1_1_0_TESTING

dataset_config = dataclasses.replace(
    dataset_config,
    max_num_objects=CONFIG['max_num_objects'],
    path=dataset_config.path.replace("///", "//")
)

# Create prefetch queue for scenarios
class ScenarioPrefetcher:
    """Prefetch scenarios in background thread for faster training."""
    
    def __init__(self, dataset_config, buffer_size=16):
        self.dataset_config = dataset_config
        self.buffer_size = buffer_size
        self.queue = queue.Queue(maxsize=buffer_size)
        self.stop_event = threading.Event()
        self.thread = None
        
    def _worker(self):
        """Background worker to prefetch scenarios."""
        iterator = waymax_dataloader.simulator_state_generator(self.dataset_config)
        while not self.stop_event.is_set():
            try:
                scenario = next(iterator)
                self.queue.put(scenario, timeout=1.0)
            except StopIteration:
                iterator = waymax_dataloader.simulator_state_generator(self.dataset_config)
            except queue.Full:
                continue
                    
    def start(self):
        """Start prefetch thread."""
        self.thread = threading.Thread(target=self._worker, daemon=True)
        self.thread.start()
        print(f"âœ… Started prefetch thread (buffer size: {self.buffer_size})")
        
    def get(self):
        """Get next scenario from buffer."""
        return self.queue.get(timeout=60.0)
    
    def stop(self):
        """Stop prefetch thread."""
        self.stop_event.set()
        if self.thread:
            self.thread.join(timeout=2.0)

prefetcher = ScenarioPrefetcher(dataset_config, CONFIG['prefetch_buffer_size'])
prefetcher.start()

# Also create regular iterator for initial test
data_iterator = waymax_dataloader.simulator_state_generator(dataset_config)

print(f"âœ… Waymax data loader ready with prefetch")
print(f"   Streaming from: {dataset_config.path}")

# %% [markdown]
# ## 7. Test Data Loading

# %%
print("ðŸ” Testing data loading...")
sample_scenario = next(data_iterator)
print(f"âœ… Scenario loaded! Num objects: {sample_scenario.num_objects}")

# %% [markdown]
# ## 8. Initialize Environment with JIT

# %%
print("ðŸ”§ Initializing Waymax environment with JIT compilation...")

dynamics_model = waymax_dynamics.StateDynamics()
env_config = waymax_config.EnvironmentConfig(
    max_num_objects=CONFIG['max_num_objects'],
    controlled_object=waymax_config.ObjectType.VALID,
)

sim_env = waymax_env.BaseEnvironment(
    dynamics_model=dynamics_model,
    config=env_config,
)

# JIT compile the step function for faster execution
if CONFIG['use_jit']:
    jit_step = jax.jit(sim_env.step)
    jit_reset = jax.jit(sim_env.reset)
    print("âœ… JIT compiled step and reset functions")
else:
    jit_step = sim_env.step
    jit_reset = sim_env.reset

print(f"âœ… Waymax environment ready")

# %% [markdown]
# ## 9. Optimized Helper Functions

# %%
def select_controlled_agents(scenario, num_agents):
    """Select agents to control."""
    valid_mask = np.array(scenario.object_metadata.is_valid)
    is_sdc = np.array(scenario.object_metadata.is_sdc)
    obj_types = np.array(scenario.object_metadata.object_types)
    vehicle_mask = (obj_types == 1) & valid_mask & ~is_sdc
    vehicle_indices = np.where(vehicle_mask)[0]
    if len(vehicle_indices) >= num_agents:
        return vehicle_indices[:num_agents].tolist()
    all_valid = np.where(valid_mask)[0]
    return all_valid[:num_agents].tolist()


def extract_observations_batch(state, agent_indices, goal_cache=None, history_buffer=None):
    """Extract RICH observations for RL agents.
    
    Features per agent (39 total):
    - Self state (7): x, y, vx, vy, yaw, speed, length (normalized)
    - Relative to other 3 agents (12): dx, dy, dist for each
    - Goal info (4): dx_goal, dy_goal, dist_goal, heading_to_goal
    - History embedding (16): velocity changes over last 4 steps
    """
    timestep = int(np.array(state.timestep))
    num_agents = len(agent_indices)
    
    # Extract all agent data at once
    x_all = np.array(state.sim_trajectory.x[agent_indices, timestep])
    y_all = np.array(state.sim_trajectory.y[agent_indices, timestep])
    vx_all = np.array(state.sim_trajectory.vel_x[agent_indices, timestep])
    vy_all = np.array(state.sim_trajectory.vel_y[agent_indices, timestep])
    yaw_all = np.array(state.sim_trajectory.yaw[agent_indices, timestep])
    
    # Goals from log trajectory endpoint
    goal_x = np.array([state.log_trajectory.x[idx, -1] for idx in agent_indices])
    goal_y = np.array([state.log_trajectory.y[idx, -1] for idx in agent_indices])
    
    observations = []
    
    for i in range(num_agents):
        obs_features = []
        
        # === 1. SELF STATE (7 features) ===
        ref_x, ref_y = x_all[0], y_all[0]  # Reference = first agent
        speed = np.sqrt(vx_all[i]**2 + vy_all[i]**2)
        obs_features.extend([
            np.clip((x_all[i] - ref_x) / 100.0, -10, 10),  # Relative x
            np.clip((y_all[i] - ref_y) / 100.0, -10, 10),  # Relative y
            np.clip(vx_all[i] / 30.0, -2, 2),              # Velocity x
            np.clip(vy_all[i] / 30.0, -2, 2),              # Velocity y
            np.clip(yaw_all[i] / np.pi, -1, 1),            # Yaw
            np.clip(speed / 30.0, 0, 2),                   # Speed
            0.45,                                           # Length (normalized)
        ])
        
        # === 2. RELATIVE TO OTHER AGENTS (12 features = 4 agents * 3) ===
        for j in range(num_agents):
            if j != i:
                dx = (x_all[j] - x_all[i]) / 50.0  # Closer range scale
                dy = (y_all[j] - y_all[i]) / 50.0
                dist = np.sqrt(dx**2 + dy**2)
                obs_features.extend([
                    np.clip(dx, -5, 5),
                    np.clip(dy, -5, 5),
                    np.clip(dist, 0, 7),
                ])
        # Pad if less than 3 other agents
        for _ in range(3 - (num_agents - 1)):
            obs_features.extend([0.0, 0.0, 10.0])  # Far away placeholder
        
        # === 3. GOAL INFO (4 features) ===
        dx_goal = (goal_x[i] - x_all[i]) / 100.0
        dy_goal = (goal_y[i] - y_all[i]) / 100.0
        dist_goal = np.sqrt(dx_goal**2 + dy_goal**2)
        heading_to_goal = np.arctan2(dy_goal, dx_goal) - yaw_all[i]
        heading_to_goal = np.clip(heading_to_goal / np.pi, -1, 1)
        obs_features.extend([
            np.clip(dx_goal, -10, 10),
            np.clip(dy_goal, -10, 10),
            np.clip(dist_goal, 0, 15),
            heading_to_goal,
        ])
        
        # === 4. HISTORY EMBEDDING (16 features) ===
        # Use historical velocities from log trajectory if available
        hist_features = []
        for t_offset in range(1, 5):  # Last 4 timesteps
            t_hist = max(0, timestep - t_offset)
            hist_vx = float(np.array(state.sim_trajectory.vel_x[agent_indices[i], t_hist]))
            hist_vy = float(np.array(state.sim_trajectory.vel_y[agent_indices[i], t_hist]))
            hist_yaw = float(np.array(state.sim_trajectory.yaw[agent_indices[i], t_hist]))
            hist_speed = np.sqrt(hist_vx**2 + hist_vy**2)
            hist_features.extend([
                np.clip(hist_vx / 30.0, -2, 2),
                np.clip(hist_vy / 30.0, -2, 2),
                np.clip(hist_yaw / np.pi, -1, 1),
                np.clip(hist_speed / 30.0, 0, 2),
            ])
        obs_features.extend(hist_features)
        
        observations.append(obs_features)
    
    obs_array = np.array(observations, dtype=np.float32)
    return torch.from_numpy(obs_array).to(device)


def create_action_batch(actions_list, agent_indices, state):
    """Create Waymax action - optimized version."""
    num_objects = int(np.array(state.num_objects))
    timestep = int(np.array(state.timestep))
    dt = 0.1
    
    # Batch extract current state
    current_x = np.array(state.sim_trajectory.x[:, timestep])
    current_y = np.array(state.sim_trajectory.y[:, timestep])
    current_yaw = np.array(state.sim_trajectory.yaw[:, timestep])
    current_vel_x = np.array(state.sim_trajectory.vel_x[:, timestep])
    current_vel_y = np.array(state.sim_trajectory.vel_y[:, timestep])
    
    next_x = current_x.copy()
    next_y = current_y.copy()
    next_yaw = current_yaw.copy()
    next_vel_x = current_vel_x.copy()
    next_vel_y = current_vel_y.copy()
    action_valid = np.zeros((num_objects,), dtype=bool)
    
    for i, agent_idx in enumerate(agent_indices):
        if i < len(actions_list):
            action = actions_list[i].detach().cpu().numpy().flatten()
            accel = np.clip(action[0] if len(action) > 0 else 0.0, -4.0, 4.0)
            yaw_rate = np.clip(action[1] if len(action) > 1 else 0.0, -0.5, 0.5)
            
            speed = np.sqrt(current_vel_x[agent_idx]**2 + current_vel_y[agent_idx]**2)
            next_yaw[agent_idx] = current_yaw[agent_idx] + yaw_rate * dt
            new_speed = max(0, speed + accel * dt)
            next_vel_x[agent_idx] = new_speed * np.cos(next_yaw[agent_idx])
            next_vel_y[agent_idx] = new_speed * np.sin(next_yaw[agent_idx])
            avg_vel_x = (current_vel_x[agent_idx] + next_vel_x[agent_idx]) / 2
            avg_vel_y = (current_vel_y[agent_idx] + next_vel_y[agent_idx]) / 2
            next_x[agent_idx] = current_x[agent_idx] + avg_vel_x * dt
            next_y[agent_idx] = current_y[agent_idx] + avg_vel_y * dt
            action_valid[agent_idx] = True
    
    traj_update = datatypes.TrajectoryUpdate(
        x=jnp.array(next_x)[:, jnp.newaxis],
        y=jnp.array(next_y)[:, jnp.newaxis],
        yaw=jnp.array(next_yaw)[:, jnp.newaxis],
        vel_x=jnp.array(next_vel_x)[:, jnp.newaxis],
        vel_y=jnp.array(next_vel_y)[:, jnp.newaxis],
        valid=jnp.array(action_valid)[:, jnp.newaxis],
    )
    return traj_update.as_action()


def compute_rewards_batch(state, agent_indices, distance_limit=5.0):
    """Compute rewards for all agents at once - vectorized."""
    timestep = int(np.array(state.timestep))
    num_agents = len(agent_indices)
    
    # Current positions
    x = np.array([state.sim_trajectory.x[idx, timestep] for idx in agent_indices])
    y = np.array([state.sim_trajectory.y[idx, timestep] for idx in agent_indices])
    
    # Goal positions
    goal_x = np.array([state.log_trajectory.x[idx, -1] for idx in agent_indices])
    goal_y = np.array([state.log_trajectory.y[idx, -1] for idx in agent_indices])
    
    # Distance to goal reward
    dist_to_goal = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
    rewards = -dist_to_goal * 0.1
    
    # Collision check - vectorized pairwise distances
    costs = np.zeros(num_agents)
    for i in range(num_agents):
        for j in range(i+1, num_agents):
            dist = np.sqrt((x[i] - x[j])**2 + (y[i] - y[j])**2)
            if dist < 2.0:
                rewards[i] -= 10.0
                rewards[j] -= 10.0
            if dist < distance_limit:
                costs[i] = 1.0
                costs[j] = 1.0
    
    return rewards, costs


def create_empty_transition(num_agents):
    """Create empty transition dict."""
    return {
        "observations": [[] for _ in range(num_agents)],
        "actions": [[] for _ in range(num_agents)],
        "next_observations": [[] for _ in range(num_agents)],
        "rewards": [[] for _ in range(num_agents)],
        "costs": [[] for _ in range(num_agents)],
        "magnet": [[] for _ in range(num_agents)],
        "dones": [],
    }

# %% [markdown]
# ## 10. Build RL Config

# %%
print("ðŸ“‹ Building RL configuration...")

rl_config = {
    'algorithm': CONFIG['rl_algorithm'],
    'batch_size': CONFIG['batch_size'],
    'gamma': CONFIG['gamma'],
    'lamda': CONFIG['gae_lambda'],
    'actor_learning_rate': CONFIG['actor_lr'],
    'critic_learning_rate': CONFIG['critic_lr'],
    'eps': CONFIG['eps'],
    'entropy_coef': CONFIG['entropy_coef'],
    'epochs': CONFIG['epochs'],
    'hidden_dim': CONFIG['hidden_dim'],
    'agent_number': CONFIG['num_controlled_agents'],
    'penalty_initial_value': CONFIG['penalty_initial_value'],
    'cost_quantile': CONFIG['cost_quantile'],
    'constrainted_critic_learning_rate': 1e-4,
    'is_magnet': CONFIG['is_magnet'],
    'eta_coef1': CONFIG['eta_coef1'],
    'eta_coef2': 0.05,
    'beta_coef': 0.1,
    'N_quantile': 64,
    'tau_update': 0.01,
    'LR_QN': 3e-4,
    'type': 'CVaR',
    'method': 'SplineDQN',
    'offset': 5,
    'target_kl': 0.01,
    'density_learning_rate': 3e-4,
    'gae': True,
}

print("âœ… RL Config ready")

# %% [markdown]
# ## 11. Initialize RL Agents

# %%
print("ðŸ¤– Initializing RL agents...")

NUM_AGENTS = CONFIG['num_controlled_agents']
STATE_DIM = CONFIG['obs_dim']
BATCH_SIZE = CONFIG['batch_size']

if CONFIG['rl_algorithm'] == 'TrafficGamer':
    agents = [TrafficGamer(STATE_DIM, NUM_AGENTS, rl_config, device) for _ in range(NUM_AGENTS)]
    print(f"âœ… Initialized {NUM_AGENTS} TrafficGamer agents")
elif CONFIG['rl_algorithm'] == 'Constrainted_CCE_MAPPO':
    agents = [Constrainted_CCE_MAPPO(STATE_DIM, NUM_AGENTS, rl_config, device) for _ in range(NUM_AGENTS)]
    print(f"âœ… Initialized {NUM_AGENTS} Constrainted CCE-MAPPO agents")
else:
    agents = [MAPPO(STATE_DIM, NUM_AGENTS, rl_config, device) for _ in range(NUM_AGENTS)]
    print(f"âœ… Initialized {NUM_AGENTS} MAPPO agents")

# Move agents to GPU and enable cudnn benchmark
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    print("âœ… Enabled cuDNN benchmark for faster training")

# %% [markdown]
# ## 12. Optimized Training Loop

# %%
print("ðŸš€ Starting optimized training...")
print(f"   Algorithm: {CONFIG['rl_algorithm']}")
print(f"   Total Episodes: {CONFIG['num_episodes']}")
print(f"   Batch Size: {BATCH_SIZE} episodes")
print(f"   Prefetch Buffer: {CONFIG['prefetch_buffer_size']}")
print(f"   JIT Compilation: {'âœ…' if CONFIG['use_jit'] else 'âŒ'}")

metrics = {
    'episode_rewards': [],
    'episode_costs': [],
    'collision_rates': [],
    'batch_times': [],
}

# %%
import time

num_batches = CONFIG['num_episodes'] // BATCH_SIZE
episode_count = 0

for batch_idx in tqdm(range(num_batches), desc="Training"):
    batch_start_time = time.time()
    
    transition_batch = []
    batch_rewards = []
    batch_costs = []
    batch_collisions = []
    
    for ep_in_batch in range(BATCH_SIZE):
        # Use prefetcher for faster data loading
        try:
            scenario = prefetcher.get()
        except queue.Empty:
            scenario = next(data_iterator)
        
        controlled_agents = select_controlled_agents(scenario, NUM_AGENTS)
        
        if len(controlled_agents) < NUM_AGENTS:
            transition = create_empty_transition(NUM_AGENTS)
            for i in range(NUM_AGENTS):
                dummy_obs = torch.zeros(STATE_DIM, device=device)
                dummy_action = torch.zeros(2, device=device)
                transition["observations"][i].append(dummy_obs)
                transition["next_observations"][i].append(dummy_obs)
                transition["actions"][i].append(dummy_action)
                transition["rewards"][i].append(torch.tensor([0.0], device=device))
                transition["costs"][i].append(torch.tensor([0.0], device=device))
                transition["magnet"][i].append(Normal(dummy_action, torch.ones_like(dummy_action)))
            transition["dones"].append(torch.tensor(1, device=device))
            transition_batch.append(transition)
            batch_rewards.append(0.0)
            batch_costs.append(0.0)
            batch_collisions.append(0)
            continue
        
        # Use JIT-compiled reset
        state = jit_reset(scenario)
        transition = create_empty_transition(NUM_AGENTS)
        episode_rewards = np.zeros(NUM_AGENTS)
        episode_costs = np.zeros(NUM_AGENTS)
        collisions = 0
        
        for step in range(CONFIG['steps_per_episode']):
            # Optimized batch observation extraction
            obs = extract_observations_batch(state, controlled_agents)
            
            # Get actions
            actions = []
            action_dists = []
            with torch.no_grad():
                for i, agent in enumerate(agents):
                    action = agent.choose_action(obs[i].unsqueeze(0))
                    actions.append(action.squeeze(0) if action.dim() > 1 else action)
                    action_dist = agent.get_action_dist(obs[i].unsqueeze(0))
                    action_dists.append(action_dist)
            
            waymax_action = create_action_batch(actions, controlled_agents, state)
            
            # Use JIT-compiled step
            next_state = jit_step(state, waymax_action)
            
            # Batch compute rewards and costs
            rewards, costs = compute_rewards_batch(next_state, controlled_agents, CONFIG['distance_limit'])
            
            for i in range(NUM_AGENTS):
                transition["observations"][i].append(obs[i])
                transition["actions"][i].append(actions[i])
                transition["rewards"][i].append(torch.tensor([rewards[i]], device=device))
                transition["costs"][i].append(torch.tensor([costs[i]], device=device))
                transition["magnet"][i].append(action_dists[i])
                episode_rewards[i] += rewards[i]
                episode_costs[i] += costs[i]
                if costs[i] > 0:
                    collisions += 1
            
            next_obs = extract_observations_batch(next_state, controlled_agents)
            for i in range(NUM_AGENTS):
                transition["next_observations"][i].append(next_obs[i])
            
            is_done = bool(np.array(state.is_done)) if hasattr(state, 'is_done') else False
            done_tensor = torch.tensor(
                1 if is_done or step == CONFIG['steps_per_episode'] - 1 else 0, device=device)
            transition["dones"].append(done_tensor)
            
            if is_done:
                break
            state = next_state
        
        transition_batch.append(transition)
        batch_rewards.append(np.mean(episode_rewards))
        batch_costs.append(np.mean(episode_costs))
        batch_collisions.append(collisions / (CONFIG['steps_per_episode'] * NUM_AGENTS))
        episode_count += 1
    
    # Update agents
    for i, agent in enumerate(agents):
        try:
            agent.update(transition_batch, i)
        except Exception as e:
            if batch_idx == 0:
                import traceback
                print(f"âš ï¸ Update error (agent {i}): {e}")
                traceback.print_exc()
    
    batch_time = time.time() - batch_start_time
    
    avg_reward = np.mean(batch_rewards)
    avg_cost = np.mean(batch_costs)
    avg_collision = np.mean(batch_collisions)
    
    metrics['episode_rewards'].extend(batch_rewards)
    metrics['episode_costs'].extend(batch_costs)
    metrics['collision_rates'].extend(batch_collisions)
    metrics['batch_times'].append(batch_time)
    
    if (batch_idx + 1) % CONFIG['log_interval'] == 0:
        episodes_per_sec = BATCH_SIZE / batch_time
        print(f"\nðŸ“Š Batch {batch_idx + 1}/{num_batches}")
        print(f"   Reward: {avg_reward:.2f} | Cost: {avg_cost:.2f} | Collision: {avg_collision:.3f}")
        print(f"   âš¡ Speed: {episodes_per_sec:.1f} ep/s | Batch time: {batch_time:.1f}s")

# Cleanup prefetcher
prefetcher.stop()

print("\nâœ… Training completed!")
print(f"   Total episodes: {episode_count}")
print(f"   Avg batch time: {np.mean(metrics['batch_times']):.1f}s")
print(f"   Avg episodes/sec: {BATCH_SIZE / np.mean(metrics['batch_times']):.1f}")

# %% [markdown]
# ## 13. Visualize Results

# %%
print("ðŸ“ˆ Plotting results...")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

ax = axes[0, 0]
ax.plot(metrics['episode_rewards'], alpha=0.6)
window = min(50, len(metrics['episode_rewards']))
if window > 1:
    ma = pd.Series(metrics['episode_rewards']).rolling(window=window).mean()
    ax.plot(ma, color='red', linewidth=2)
ax.set_title('Episode Rewards', fontweight='bold')
ax.grid(True, alpha=0.3)

ax = axes[0, 1]
ax.plot(metrics['episode_costs'], alpha=0.6, color='orange')
ax.set_title('Episode Costs', fontweight='bold')
ax.grid(True, alpha=0.3)

ax = axes[1, 0]
ax.plot(metrics['collision_rates'], alpha=0.6, color='red')
ax.set_title('Collision Rate', fontweight='bold')
ax.grid(True, alpha=0.3)

ax = axes[1, 1]
ax.plot(metrics['batch_times'], alpha=0.6, color='green')
ax.axhline(np.mean(metrics['batch_times']), color='red', linestyle='--', label='Mean')
ax.set_title('Batch Time (seconds)', fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_progress_v6.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… Saved to 'training_progress_v6.png'")

# %% [markdown]
# ## 14. Save Models

# %%
if CONFIG['save_checkpoint']:
    print("ðŸ’¾ Saving models...")
    save_dir = Path('checkpoints')
    save_dir.mkdir(exist_ok=True)
    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
    
    for i, agent in enumerate(agents):
        checkpoint = {
            'agent_index': i,
            'state_dim': STATE_DIM,
            'agent_number': NUM_AGENTS,
            'config': rl_config,
            'policy_state_dict': agent.pi.state_dict(),
            'value_state_dict': agent.value.state_dict(),
        }
        path = save_dir / f'{CONFIG["rl_algorithm"]}_agent{i}_{timestamp}.pt'
        torch.save(checkpoint, path)
        print(f"   âœ… Agent {i}: {path}")
    
    metrics_path = save_dir / f'metrics_{timestamp}.pkl'
    with open(metrics_path, 'wb') as f:
        pickle.dump({'metrics': metrics, 'config': CONFIG}, f)
    print(f"   âœ… Metrics: {metrics_path}")

# %% [markdown]
# ## Summary
#
# **v6 Optimizations:**
# - âœ… Background prefetch thread for data loading
# - âœ… JIT-compiled simulation functions
# - âœ… Vectorized observation extraction
# - âœ… Vectorized reward/cost computation
# - âœ… cuDNN benchmark enabled
# - âœ… Larger batch size (64 vs 32)
# - âœ… Performance monitoring (episodes/sec)
