# %% [markdown]
# # üöó TrafficGamer - Pure Waymax Training on Kaggle (v5 - All Fixes + Magnet)
#
# **Train multi-agent RL policies using Waymax simulator**
#
# **Key Features:**
# - ‚úÖ NO local data needed - streams directly from Google Cloud
# - ‚úÖ NO QCNet needed - uses Waymax observations directly
# - ‚úÖ Waymax handles: data loading + dynamics + simulation
# - ‚úÖ Train TrafficGamer / MAPPO / CCE-MAPPO policies
# - ‚úÖ **FIXED v2: Proper multi-agent environment (BaseEnvironment)**
# - ‚úÖ **FIXED v3: Proper batch collection for agent.update()**
# - ‚úÖ **FIXED v5: Fixed TrafficGamer.py magnet bugs - now magnet works!**
#
# **Repository:** https://github.com/PhamPhuHoa-23/EvolutionaryTest

# %% [markdown]
# ## 1. Install Dependencies

# %%
# Core dependencies
!pip install -q torch torchvision torchaudio
!pip install -q pytorch-lightning==2.0.0
!pip install -q torch-geometric

# %%
# Waymax - JAX-based simulator for Waymo (includes data loading)
!pip install --upgrade pip
!pip install git+https://github.com/waymo-research/waymax.git@main

# %%
# TensorFlow for data loading (Waymax uses tf.data internally)
!pip install -q tensorflow

# %%
import os
import sys
import warnings
from pathlib import Path
import yaml
import pickle
import dataclasses
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Laplace, Normal
import numpy as np
import tensorflow as tf
import jax.numpy as jnp

print(f"PyTorch version: {torch.__version__}")
print(f"TensorFlow version: {tf.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    cuda_version = torch.version.cuda.replace('.', '')
    print(f"CUDA version: {cuda_version}")
else:
    cuda_version = 'cpu'

# %%
# PyTorch Geometric dependencies
if torch.cuda.is_available():
    !pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cu{cuda_version[:3]}.html
else:
    !pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cpu.html

# %% [markdown]
# ## 2. Setup Environment

# %%
warnings.filterwarnings('ignore')
tf.config.set_visible_devices([], 'GPU')  # Disable TF GPU (use PyTorch)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"‚úÖ Using device: {device}")

# %% [markdown]
# ## 3. Authenticate with Google Cloud
#
# **REQUIRED** to access Waymo Open Motion Dataset from GCS bucket
#
# Choose ONE method:
# - **Kaggle**: Upload service account key as dataset
# - **Colab**: Run `auth.authenticate_user()`

# %%
print("üîë Authenticating with Google Cloud...")

# Method 1: Service Account Key (Kaggle)
service_key_path = '/kaggle/input/gcs-credentials/auth.json'

if os.path.exists(service_key_path):
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = service_key_path
    print(f"‚úÖ Authenticated via service account")
else:
    # Method 2: Colab authentication
    try:
        from google.colab import auth
        auth.authenticate_user()
        print("‚úÖ Authenticated via Colab")
    except ImportError:
        print("‚ö†Ô∏è  No authentication found!")
        print("   Kaggle: Upload GCS credentials as dataset")
        print("   Colab: from google.colab import auth; auth.authenticate_user()")

# %% [markdown]
# ## 4. Clone Repository & Import Components

# %%
REPO_DIR = Path("EvolutionaryTest")

if not REPO_DIR.exists():
    print("üì• Cloning repository...")
    !git clone https://github.com/PhamPhuHoa-23/EvolutionaryTest.git
else:
    print("‚úÖ Repository exists, pulling latest...")
    !cd EvolutionaryTest && git pull

sys.path.insert(0, str(REPO_DIR.absolute()))
os.chdir(REPO_DIR)
print(f"üìÅ Working directory: {os.getcwd()}")

# %% [markdown]
# ## 4.1 Apply Bug Fixes to TrafficGamer
#
# Fix two bugs in TrafficGamer.py:
# 1. `log_prob()` returns multi-dim tensor but needs `.sum()` to convert to scalar
# 2. `magnet_signal` is accessed before being defined when `is_magnet=False`

# %%
print("üîß Applying bug fixes to TrafficGamer.py...")

trafficgamer_path = Path("algorithm/TrafficGamer.py")
if trafficgamer_path.exists():
    content = trafficgamer_path.read_text()
    
    # Check if already patched with the latest fix (looks for .item())
    if "log_prob(actions[i]).sum().item()" not in content:
        # Fix 1: Add .sum().item() to log_prob and dtype=torch.float32
        # Replace the old magnet block with the new fixed version
        old_magnet_pattern1 = "[magnet[i].log_prob(actions[i]) for i in range(len(actions))]"
        old_magnet_pattern2 = "[magnet[i].log_prob(actions[i]).sum() for i in range(len(actions))]"
        new_magnet = "[magnet[i].log_prob(actions[i]).sum().item() for i in range(len(actions))],\n                    dtype=torch.float32"
        
        if old_magnet_pattern1 in content:
            content = content.replace(old_magnet_pattern1, new_magnet)
        elif old_magnet_pattern2 in content and "dtype=torch.float32" not in content:
            content = content.replace(old_magnet_pattern2 + "\n                )", new_magnet + "\n                )")
        
        # Fix 2: Initialize magnet_signal with dtype before conditional
        if "magnet_signal = torch.zeros(len(actions))" in content and "dtype=torch.float32" not in content:
            content = content.replace(
                "magnet_signal = torch.zeros(len(actions))",
                "magnet_signal = torch.zeros(len(actions), dtype=torch.float32)"
            )
        
        # Fix 3: Add initialization if not present
        old_if_magnet = '''            if  self.magnet:
                magnet_signal = torch.tensor('''
        new_if_magnet = '''            # Initialize magnet_signal (used for logging even when magnet is disabled)
            magnet_signal = torch.zeros(len(actions), dtype=torch.float32)
            
            if self.magnet:
                # Use .sum().item() on log_prob to convert to scalar, dtype=float32 to match model
                magnet_signal = torch.tensor('''
        if old_if_magnet in content:
            content = content.replace(old_if_magnet, new_if_magnet)
        
        # Fix 4: Wrap log access in conditional
        old_log = '''log["magnet_signal"] = magnet_signal.mean().item()'''
        new_log = '''# Only log magnet_signal if magnet is enabled
            if self.magnet:
                log["magnet_signal"] = magnet_signal.mean().item()
            else:
                log["magnet_signal"] = 0.0'''
        if old_log in content:
            content = content.replace(old_log, new_log)
        
        trafficgamer_path.write_text(content)
        print("‚úÖ Applied bug fixes to TrafficGamer.py")
    else:
        print("‚úÖ TrafficGamer.py already has latest patches")
else:
    print("‚ö†Ô∏è TrafficGamer.py not found - will use original code")

# %%
!pip install -q -r requirements.txt

# %%
# Waymax imports
from waymax import config as waymax_config
from waymax import dataloader as waymax_dataloader
from waymax import env as waymax_env
from waymax import dynamics as waymax_dynamics
from waymax import datatypes
from waymax import visualization as waymax_viz

# TrafficGamer RL algorithms
from algorithm.mappo import MAPPO
from algorithm.cce_mappo import CCE_MAPPO
from algorithm.constrainted_cce_mappo import Constrainted_CCE_MAPPO
from algorithm.TrafficGamer import TrafficGamer
from utils.utils import seed_everything

import pandas as pd

print("‚úÖ All imports successful")

# %% [markdown]
# ## 5. Configuration

# %%
# ============================================
# üîß CONFIGURATION
# ============================================

CONFIG = {
    # Waymo dataset version & split
    'waymo_version': '1.2.0',  # '1.1.0' or '1.2.0'
    'split': 'training',  # 'training', 'validation', 'testing'

    # Waymax settings
    'max_num_objects': 32,  # Max objects per scenario (memory)

    # Observation settings
    'obs_dim': 7,  # [x, y, vx, vy, yaw, speed, length]

    # Training settings
    'seed': 42,
    'num_episodes': 300,
    'steps_per_episode': 80,  # 8 seconds @ 10Hz
    'batch_size': 32,  # Number of episodes to collect before update

    # RL algorithm: 'TrafficGamer', 'MAPPO', 'CCE_MAPPO', 'Constrainted_CCE_MAPPO'
    'rl_algorithm': 'TrafficGamer',

    # Agent settings
    'num_controlled_agents': 4,  # Number of agents to control
    'hidden_dim': 128,

    # RL hyperparameters
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'actor_lr': 3e-4,
    'critic_lr': 3e-4,
    'eps': 0.2,
    'entropy_coef': 0.005,
    'epochs': 10,

    # Safety constraints
    'distance_limit': 5.0,  # Collision threshold (meters)
    'cost_quantile': 48,
    'penalty_initial_value': 1.0,

    # Magnet (exploration) - NOW WORKS after TrafficGamer.py fixes!
    'is_magnet': True,
    'eta_coef1': 0.05,

    # Logging
    'log_interval': 10,
    'save_checkpoint': True,
}

seed_everything(CONFIG['seed'])

print("‚úÖ Configuration:")
for k, v in CONFIG.items():
    print(f"   {k}: {v}")

# %% [markdown]
# ## 6. Initialize Waymax Data Loader
#
# **Waymax streams data directly from Google Cloud - NO download needed!**

# %%
print("üìÇ Initializing Waymax data loader...")

# Select dataset config based on version and split
if CONFIG['waymo_version'] == '1.2.0':
    if CONFIG['split'] == 'training':
        dataset_config = waymax_config.WOD_1_2_0_TRAINING
    elif CONFIG['split'] == 'validation':
        dataset_config = waymax_config.WOD_1_2_0_VALIDATION
    else:
        dataset_config = waymax_config.WOD_1_2_0_TESTING
else:  # 1.1.0
    if CONFIG['split'] == 'training':
        dataset_config = waymax_config.WOD_1_1_0_TRAINING
    elif CONFIG['split'] == 'validation':
        dataset_config = waymax_config.WOD_1_1_0_VALIDATION
    else:
        dataset_config = waymax_config.WOD_1_1_0_TESTING

# Fix GCS path and configure
dataset_config = dataclasses.replace(
    dataset_config,
    max_num_objects=CONFIG['max_num_objects'],
    path=dataset_config.path.replace("///", "//")
)

# Create data iterator (streams from GCS)
data_iterator = waymax_dataloader.simulator_state_generator(dataset_config)

print(f"‚úÖ Waymax data loader ready")
print(f"   Dataset: Waymo Open Motion Dataset v{CONFIG['waymo_version']}")
print(f"   Split: {CONFIG['split']}")
print(f"   Streaming from: {dataset_config.path}")

# %% [markdown]
# ## 7. Test Data Loading

# %%
print("üîç Testing data loading...")

# Get first scenario
sample_scenario = next(data_iterator)

print(f"‚úÖ Scenario loaded!")
print(f"   Scenario shape: {type(sample_scenario)}")
print(f"   Num objects: {sample_scenario.num_objects}")
print(f"   Timestep: {sample_scenario.timestep}")

# Trajectory info
print(f"\nüìä Trajectory data:")
print(f"   Position shape: {sample_scenario.log_trajectory.xy.shape}")
print(f"   Velocity X shape: {sample_scenario.log_trajectory.vel_x.shape}")
print(f"   Yaw shape: {sample_scenario.log_trajectory.yaw.shape}")
print(f"   Valid shape: {sample_scenario.log_trajectory.valid.shape}")

# Object metadata
print(f"\nüöó Object metadata:")
print(f"   Object types: {np.unique(np.array(sample_scenario.object_metadata.object_types))}")
print(f"   Is SDC: {np.array(sample_scenario.object_metadata.is_sdc).sum()} SDC agents")

# %% [markdown]
# ## 8. Visualize Sample Scenario

# %%
print("üé® Visualizing scenario...")

try:
    # Waymax built-in visualization
    img = waymax_viz.plot_simulator_state(sample_scenario, use_log_traj=True)

    plt.figure(figsize=(14, 12))
    plt.imshow(img)
    plt.axis('off')
    plt.title('Waymo Scenario - Log Trajectory',
              fontsize=14, fontweight='bold')
    plt.savefig('waymax_scenario.png', dpi=150, bbox_inches='tight')
    plt.show()
    print("‚úÖ Visualization saved to 'waymax_scenario.png'")
except Exception as e:
    print(f"‚ö†Ô∏è  Visualization error: {e}")

# %% [markdown]
# ## 9. Initialize Waymax Environment (Multi-Agent Fixed)
#
# **FIXED:** Using `BaseEnvironment` instead of `PlanningAgentEnvironment` for multi-agent control.

# %%
print("üîß Initializing Waymax environment (Multi-Agent)...")

# Use StateDynamics for multi-agent control
# StateDynamics accepts direct trajectory updates, which is simpler for RL training
dynamics_model = waymax_dynamics.StateDynamics()

# Environment config - control all valid objects
env_config = waymax_config.EnvironmentConfig(
    max_num_objects=CONFIG['max_num_objects'],
    controlled_object=waymax_config.ObjectType.VALID,  # Control all valid objects
)

# Create BaseEnvironment for multi-agent simulation
# NOTE: BaseEnvironment allows controlling multiple agents, unlike PlanningAgentEnvironment
sim_env = waymax_env.BaseEnvironment(
    dynamics_model=dynamics_model,
    config=env_config,
)

print(f"‚úÖ Waymax environment ready (Multi-Agent)")
print(f"   Environment: BaseEnvironment")
print(f"   Dynamics: StateDynamics")
print(f"   Max objects: {CONFIG['max_num_objects']}")
print(f"   Controlled: All valid objects")

# %% [markdown]
# ## 10. Helper Functions

# %%
def select_controlled_agents(scenario, num_agents):
    """Select agents to control (prioritize valid, non-SDC vehicles)."""
    valid_mask = np.array(scenario.object_metadata.is_valid)
    is_sdc = np.array(scenario.object_metadata.is_sdc)
    obj_types = np.array(scenario.object_metadata.object_types)

    # Prioritize: valid vehicles that are not SDC
    # Object type 1 = VEHICLE in Waymo
    vehicle_mask = (obj_types == 1) & valid_mask & ~is_sdc
    vehicle_indices = np.where(vehicle_mask)[0]

    if len(vehicle_indices) >= num_agents:
        return vehicle_indices[:num_agents].tolist()

    # Fallback: include SDC if needed
    all_valid = np.where(valid_mask)[0]
    return all_valid[:num_agents].tolist()


def extract_observations(state, agent_indices):
    """Extract observations from Waymax state for RL agents.

    Returns: torch.Tensor of shape (num_agents, obs_dim)
    obs = [x, y, vx, vy, yaw, speed, length]
    """
    observations = []
    timestep = int(np.array(state.timestep))

    for idx in agent_indices:
        # Current position and velocity from sim_trajectory
        x = float(np.array(state.sim_trajectory.x[idx, timestep]))
        y = float(np.array(state.sim_trajectory.y[idx, timestep]))
        vx = float(np.array(state.sim_trajectory.vel_x[idx, timestep]))
        vy = float(np.array(state.sim_trajectory.vel_y[idx, timestep]))
        yaw = float(np.array(state.sim_trajectory.yaw[idx, timestep]))

        # Derived features
        speed = np.sqrt(vx**2 + vy**2)
        length = 4.5  # Default vehicle length (meters)

        obs = [x, y, vx, vy, yaw, speed, length]
        observations.append(obs)

    return torch.FloatTensor(observations).to(device)


def create_action(actions_list, agent_indices, state):
    """Convert RL actions to Waymax Action format for StateDynamics.

    StateDynamics expects trajectory updates: [x, y, yaw, vel_x, vel_y]
    RL actions: [acceleration, steering_angle] per agent
    
    We convert acceleration/steering to position/velocity changes using
    simple kinematic equations.
    """
    num_objects = int(np.array(state.num_objects))
    timestep = int(np.array(state.timestep))
    dt = 0.1  # 10Hz simulation
    
    # Get current trajectory state for all objects
    current_x = np.array(state.sim_trajectory.x[:, timestep])
    current_y = np.array(state.sim_trajectory.y[:, timestep])
    current_yaw = np.array(state.sim_trajectory.yaw[:, timestep])
    current_vel_x = np.array(state.sim_trajectory.vel_x[:, timestep])
    current_vel_y = np.array(state.sim_trajectory.vel_y[:, timestep])
    
    # Initialize with current state (no change by default - log playback for uncontrolled)
    next_x = current_x.copy()
    next_y = current_y.copy() 
    next_yaw = current_yaw.copy()
    next_vel_x = current_vel_x.copy()
    next_vel_y = current_vel_y.copy()
    action_valid = np.zeros((num_objects,), dtype=bool)
    
    for i, agent_idx in enumerate(agent_indices):
        if i < len(actions_list):
            action = actions_list[i].detach().cpu().numpy().flatten()
            
            # Interpret actions as acceleration and yaw rate (steering)
            accel = np.clip(action[0] if len(action) > 0 else 0.0, -4.0, 4.0)  # m/s^2
            yaw_rate = np.clip(action[1] if len(action) > 1 else 0.0, -0.5, 0.5)  # rad/s
            
            # Current speed
            speed = np.sqrt(current_vel_x[agent_idx]**2 + current_vel_y[agent_idx]**2)
            
            # Update yaw
            next_yaw[agent_idx] = current_yaw[agent_idx] + yaw_rate * dt
            
            # Update speed (clamp to non-negative)
            new_speed = max(0, speed + accel * dt)
            
            # Update velocity components based on new yaw
            next_vel_x[agent_idx] = new_speed * np.cos(next_yaw[agent_idx])
            next_vel_y[agent_idx] = new_speed * np.sin(next_yaw[agent_idx])
            
            # Update position using average velocity
            avg_vel_x = (current_vel_x[agent_idx] + next_vel_x[agent_idx]) / 2
            avg_vel_y = (current_vel_y[agent_idx] + next_vel_y[agent_idx]) / 2
            next_x[agent_idx] = current_x[agent_idx] + avg_vel_x * dt
            next_y[agent_idx] = current_y[agent_idx] + avg_vel_y * dt
            
            action_valid[agent_idx] = True
    
    # Create trajectory update for StateDynamics
    # Shape: (num_objects, 1) for single timestep action
    traj_update = datatypes.TrajectoryUpdate(
        x=jnp.array(next_x)[:, jnp.newaxis],
        y=jnp.array(next_y)[:, jnp.newaxis],
        yaw=jnp.array(next_yaw)[:, jnp.newaxis],
        vel_x=jnp.array(next_vel_x)[:, jnp.newaxis],
        vel_y=jnp.array(next_vel_y)[:, jnp.newaxis],
        valid=jnp.array(action_valid)[:, jnp.newaxis],
    )
    
    return traj_update.as_action()


def compute_reward(state, agent_idx, agent_indices, distance_limit=5.0):
    """Compute reward for an agent.

    Rewards:
    - Progress toward goal (log trajectory endpoint)
    - Collision penalty
    - Off-road penalty (simplified)
    """
    reward = 0.0

    timestep = int(np.array(state.timestep))

    # Current position
    x = float(np.array(state.sim_trajectory.x[agent_idx, timestep]))
    y = float(np.array(state.sim_trajectory.y[agent_idx, timestep]))

    # Goal position (end of log trajectory)
    goal_x = float(np.array(state.log_trajectory.x[agent_idx, -1]))
    goal_y = float(np.array(state.log_trajectory.y[agent_idx, -1]))

    # Distance to goal reward (negative distance)
    dist_to_goal = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)
    reward -= dist_to_goal * 0.1  # Scale factor

    # Collision penalty - check distance to other controlled agents
    for other_idx in agent_indices:
        if other_idx != agent_idx:
            other_x = float(np.array(state.sim_trajectory.x[other_idx, timestep]))
            other_y = float(np.array(state.sim_trajectory.y[other_idx, timestep]))
            dist = np.sqrt((x - other_x)**2 + (y - other_y)**2)

            if dist < 2.0:  # Collision
                reward -= 10.0
                break

    return reward


def compute_cost(state, agent_idx, agent_indices, distance_limit=5.0):
    """Compute safety cost (for constrained RL).

    Cost = 1 if too close to another agent, else 0
    """
    timestep = int(np.array(state.timestep))

    x = float(np.array(state.sim_trajectory.x[agent_idx, timestep]))
    y = float(np.array(state.sim_trajectory.y[agent_idx, timestep]))

    for other_idx in agent_indices:
        if other_idx != agent_idx:
            other_x = float(np.array(state.sim_trajectory.x[other_idx, timestep]))
            other_y = float(np.array(state.sim_trajectory.y[other_idx, timestep]))
            dist = np.sqrt((x - other_x)**2 + (y - other_y)**2)

            if dist < distance_limit:
                return 1.0

    return 0.0


def create_empty_transition(num_agents):
    """Create an empty transition dict for one episode."""
    return {
        "observations": [[] for _ in range(num_agents)],
        "actions": [[] for _ in range(num_agents)],
        "next_observations": [[] for _ in range(num_agents)],
        "rewards": [[] for _ in range(num_agents)],
        "costs": [[] for _ in range(num_agents)],
        "magnet": [[] for _ in range(num_agents)],
        "dones": [],
    }

# %% [markdown]
# ## 11. Build RL Config


# %%
print("üìã Building RL configuration...")

rl_config = {
    'algorithm': CONFIG['rl_algorithm'],
    'batch_size': CONFIG['batch_size'],
    'gamma': CONFIG['gamma'],
    'lamda': CONFIG['gae_lambda'],
    'actor_learning_rate': CONFIG['actor_lr'],
    'critic_learning_rate': CONFIG['critic_lr'],
    'eps': CONFIG['eps'],
    'entropy_coef': CONFIG['entropy_coef'],
    'epochs': CONFIG['epochs'],
    'hidden_dim': CONFIG['hidden_dim'],
    'agent_number': CONFIG['num_controlled_agents'],

    # Constrained RL settings
    'penalty_initial_value': CONFIG['penalty_initial_value'],
    'cost_quantile': CONFIG['cost_quantile'],
    'constrainted_critic_learning_rate': 1e-4,

    # Magnet settings - FIXED and enabled!
    'is_magnet': CONFIG['is_magnet'],
    'eta_coef1': CONFIG['eta_coef1'],
    'eta_coef2': 0.05,
    'beta_coef': 0.1,

    # Distribution RL
    'N_quantile': 64,
    'tau_update': 0.01,
    'LR_QN': 3e-4,
    'type': 'CVaR',
    'method': 'SplineDQN',

    # Other
    'offset': 5,
    'target_kl': 0.01,
    'density_learning_rate': 3e-4,
    'gae': True,
}

print("‚úÖ RL Config:")
for k, v in rl_config.items():
    print(f"   {k}: {v}")

# %% [markdown]
# ## 12. Initialize RL Agents

# %%
print("ü§ñ Initializing RL agents...")

NUM_AGENTS = CONFIG['num_controlled_agents']
STATE_DIM = CONFIG['obs_dim']  # Observation dimension
BATCH_SIZE = CONFIG['batch_size']  # Episodes per batch

# Initialize agents based on algorithm
if CONFIG['rl_algorithm'] == 'TrafficGamer':
    agents = [
        TrafficGamer(STATE_DIM, NUM_AGENTS, rl_config, device)
        for _ in range(NUM_AGENTS)
    ]
    print(f"‚úÖ Initialized {NUM_AGENTS} TrafficGamer agents")

elif CONFIG['rl_algorithm'] == 'Constrainted_CCE_MAPPO':
    agents = [
        Constrainted_CCE_MAPPO(STATE_DIM, NUM_AGENTS, rl_config, device)
        for _ in range(NUM_AGENTS)
    ]
    print(f"‚úÖ Initialized {NUM_AGENTS} Constrainted CCE-MAPPO agents")

elif CONFIG['rl_algorithm'] == 'CCE_MAPPO':
    agents = [
        CCE_MAPPO(STATE_DIM, NUM_AGENTS, rl_config, device)
        for _ in range(NUM_AGENTS)
    ]
    print(f"‚úÖ Initialized {NUM_AGENTS} CCE-MAPPO agents")

else:  # MAPPO
    agents = [
        MAPPO(STATE_DIM, NUM_AGENTS, rl_config, device)
        for _ in range(NUM_AGENTS)
    ]
    print(f"‚úÖ Initialized {NUM_AGENTS} MAPPO agents")

# %% [markdown]
# ## 13. Training Loop (Fixed Batch Collection)
#
# **All fixes applied:**
# - v2: BaseEnvironment for multi-agent
# - v3: Batch collection for agent.update()
# - v5: TrafficGamer.py magnet bugs fixed

# %%
print("üöÄ Starting training...")
print(f"   Algorithm: {CONFIG['rl_algorithm']}")
print(f"   Total Episodes: {CONFIG['num_episodes']}")
print(f"   Batch Size: {BATCH_SIZE} episodes")
print(f"   Steps per episode: {CONFIG['steps_per_episode']}")
print(f"   Controlled agents: {NUM_AGENTS}")
print(f"   Updates per training: {CONFIG['num_episodes'] // BATCH_SIZE}")
print(f"   Magnet exploration: {'‚úÖ Enabled' if CONFIG['is_magnet'] else '‚ùå Disabled'}")

# Training metrics
metrics = {
    'episode_rewards': [],
    'episode_costs': [],
    'collision_rates': [],
}

# Recreate iterator for training
data_iterator = waymax_dataloader.simulator_state_generator(dataset_config)

# %%
# Main training loop - collect batch_size episodes, then update
num_batches = CONFIG['num_episodes'] // BATCH_SIZE
episode_count = 0

for batch_idx in tqdm(range(num_batches), desc="Training Batches"):
    
    # Transition buffer for this batch - needs batch_size episodes
    transition_batch = []
    batch_rewards = []
    batch_costs = []
    batch_collisions = []
    
    # Collect batch_size episodes
    for ep_in_batch in range(BATCH_SIZE):
        # Get new scenario
        try:
            scenario = next(data_iterator)
        except StopIteration:
            # Restart iterator
            data_iterator = waymax_dataloader.simulator_state_generator(dataset_config)
            scenario = next(data_iterator)

        # Select agents to control
        controlled_agents = select_controlled_agents(scenario, NUM_AGENTS)

        if len(controlled_agents) < NUM_AGENTS:
            # Not enough agents, create a dummy transition with zeros
            transition = create_empty_transition(NUM_AGENTS)
            for i in range(NUM_AGENTS):
                # Add minimal data so the batch structure is preserved
                dummy_obs = torch.zeros(STATE_DIM, device=device)
                dummy_action = torch.zeros(2, device=device)
                transition["observations"][i].append(dummy_obs)
                transition["next_observations"][i].append(dummy_obs)
                transition["actions"][i].append(dummy_action)
                transition["rewards"][i].append(torch.tensor([0.0], device=device))
                transition["costs"][i].append(torch.tensor([0.0], device=device))
                # Magnet: use a dummy distribution
                transition["magnet"][i].append(Normal(dummy_action, torch.ones_like(dummy_action)))
            transition["dones"].append(torch.tensor(1, device=device))
            transition_batch.append(transition)
            batch_rewards.append(0.0)
            batch_costs.append(0.0)
            batch_collisions.append(0)
            continue

        # Reset environment
        state = sim_env.reset(scenario)

        # Initialize transition storage for this episode
        transition = create_empty_transition(NUM_AGENTS)

        episode_rewards = [0.0] * NUM_AGENTS
        episode_costs = [0.0] * NUM_AGENTS
        collisions = 0

        # Rollout episode
        for step in range(CONFIG['steps_per_episode']):
            # Extract observations
            obs = extract_observations(state, controlled_agents)

            # Get actions from agents
            actions = []
            action_dists = []
            for i, agent in enumerate(agents):
                # Use choose_action for inference (no gradient)
                action = agent.choose_action(obs[i].unsqueeze(0))
                actions.append(action.squeeze(0) if action.dim() > 1 else action)
                # Get distribution for magnet
                action_dist = agent.get_action_dist(obs[i].unsqueeze(0))
                action_dists.append(action_dist)

            # Create Waymax action
            waymax_action = create_action(actions, controlled_agents, state)

            # Step environment
            next_state = sim_env.step(state, waymax_action)

            # Compute rewards and costs
            for i, agent_idx in enumerate(controlled_agents):
                reward = compute_reward(
                    next_state, agent_idx, controlled_agents, CONFIG['distance_limit'])
                cost = compute_cost(next_state, agent_idx,
                                   controlled_agents, CONFIG['distance_limit'])

                # Store transitions
                transition["observations"][i].append(obs[i])
                transition["actions"][i].append(actions[i])
                transition["rewards"][i].append(torch.tensor([reward], device=device))
                transition["costs"][i].append(torch.tensor([cost], device=device))
                # Store action distribution for magnet
                transition["magnet"][i].append(action_dists[i])

                episode_rewards[i] += reward
                episode_costs[i] += cost

                if cost > 0:
                    collisions += 1

            # Next observations
            next_obs = extract_observations(next_state, controlled_agents)
            for i in range(NUM_AGENTS):
                transition["next_observations"][i].append(next_obs[i])

            # Done flag
            is_done = bool(np.array(state.is_done)) if hasattr(state, 'is_done') else False
            done_tensor = torch.tensor(
                1 if is_done or step == CONFIG['steps_per_episode'] - 1 else 0, device=device)
            transition["dones"].append(done_tensor)

            if is_done:
                break

            state = next_state

        # Add this episode's transition to batch
        transition_batch.append(transition)
        batch_rewards.append(np.mean(episode_rewards))
        batch_costs.append(np.mean(episode_costs))
        batch_collisions.append(collisions / (CONFIG['steps_per_episode'] * NUM_AGENTS))
        episode_count += 1

    # Update agents with the full batch
    for i, agent in enumerate(agents):
        try:
            agent.update(transition_batch, i)
        except Exception as e:
            if batch_idx == 0:
                print(f"‚ö†Ô∏è  Update error (agent {i}): {e}")

    # Record metrics for this batch
    avg_reward = np.mean(batch_rewards)
    avg_cost = np.mean(batch_costs)
    avg_collision = np.mean(batch_collisions)

    metrics['episode_rewards'].extend(batch_rewards)
    metrics['episode_costs'].extend(batch_costs)
    metrics['collision_rates'].extend(batch_collisions)

    # Log progress
    if (batch_idx + 1) % max(1, (CONFIG['log_interval'] // BATCH_SIZE)) == 0:
        print(f"\nüìä Batch {batch_idx + 1}/{num_batches} (Episode {episode_count})")
        print(f"   Avg Reward: {avg_reward:.4f}")
        print(f"   Avg Cost: {avg_cost:.4f}")
        print(f"   Collision Rate: {avg_collision:.4f}")

print("\n‚úÖ Training completed!")
print(f"   Total episodes: {episode_count}")
print(f"   Total batches: {num_batches}")

# %% [markdown]
# ## 14. Visualize Training Progress

# %%
print("üìà Plotting training progress...")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Episode rewards
ax = axes[0, 0]
ax.plot(metrics['episode_rewards'], alpha=0.6, label='Raw')
window = min(20, len(metrics['episode_rewards']))
if window > 1:
    ma = pd.Series(metrics['episode_rewards']).rolling(window=window).mean()
    ax.plot(ma, color='red', linewidth=2, label=f'{window}-ep MA')
ax.set_title('Episode Rewards', fontweight='bold')
ax.set_xlabel('Episode')
ax.set_ylabel('Reward')
ax.legend()
ax.grid(True, alpha=0.3)

# Episode costs
ax = axes[0, 1]
ax.plot(metrics['episode_costs'], alpha=0.6, color='orange')
ax.set_title('Episode Safety Costs', fontweight='bold')
ax.set_xlabel('Episode')
ax.set_ylabel('Cost')
ax.grid(True, alpha=0.3)

# Collision rate
ax = axes[1, 0]
ax.plot(metrics['collision_rates'], alpha=0.6, color='red')
ax.set_title('Collision Rate', fontweight='bold')
ax.set_xlabel('Episode')
ax.set_ylabel('Rate')
ax.grid(True, alpha=0.3)

# Summary
ax = axes[1, 1]
ax.axis('off')
summary_episodes = min(50, len(metrics['episode_rewards']))
summary = f"""
Training Summary
================
Algorithm: {CONFIG['rl_algorithm']}
Episodes: {len(metrics['episode_rewards'])}
Agents: {NUM_AGENTS}
Batch Size: {BATCH_SIZE}
Magnet: {'Enabled' if CONFIG['is_magnet'] else 'Disabled'}

Final Metrics (last {summary_episodes} ep):
- Avg Reward: {np.mean(metrics['episode_rewards'][-summary_episodes:]):.4f}
- Avg Cost: {np.mean(metrics['episode_costs'][-summary_episodes:]):.4f}
- Collision Rate: {np.mean(metrics['collision_rates'][-summary_episodes:]):.4f}

Best Reward: {max(metrics['episode_rewards']) if metrics['episode_rewards'] else 0:.4f}
"""
ax.text(0.1, 0.5, summary, fontsize=11, family='monospace',
        verticalalignment='center', transform=ax.transAxes)

plt.tight_layout()
plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úÖ Saved to 'training_progress.png'")

# %% [markdown]
# ## 15. Save Models

# %%
if CONFIG['save_checkpoint']:
    print("üíæ Saving models...")

    save_dir = Path('checkpoints')
    save_dir.mkdir(exist_ok=True)

    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')

    for i, agent in enumerate(agents):
        checkpoint = {
            'agent_index': i,
            'state_dim': STATE_DIM,
            'agent_number': NUM_AGENTS,
            'config': rl_config,
            'policy_state_dict': agent.pi.state_dict(),
            'value_state_dict': agent.value.state_dict(),
        }

        path = save_dir / f'{CONFIG["rl_algorithm"]}_agent{i}_{timestamp}.pt'
        torch.save(checkpoint, path)
        print(f"   ‚úÖ Agent {i}: {path}")

    # Save metrics
    metrics_path = save_dir / f'metrics_{timestamp}.pkl'
    with open(metrics_path, 'wb') as f:
        pickle.dump({'metrics': metrics, 'config': CONFIG}, f)
    print(f"   ‚úÖ Metrics: {metrics_path}")

# %% [markdown]
# ## 16. Evaluate Policy

# %%
print("üîç Evaluating trained policy...")

NUM_EVAL = 20
eval_rewards = []
eval_costs = []

# New iterator for evaluation
eval_iterator = waymax_dataloader.simulator_state_generator(dataset_config)

for ep in tqdm(range(NUM_EVAL), desc="Evaluating"):
    try:
        scenario = next(eval_iterator)
    except StopIteration:
        eval_iterator = waymax_dataloader.simulator_state_generator(dataset_config)
        scenario = next(eval_iterator)

    controlled_agents = select_controlled_agents(scenario, NUM_AGENTS)
    if len(controlled_agents) < NUM_AGENTS:
        continue

    state = sim_env.reset(scenario)
    ep_reward = 0.0
    ep_cost = 0.0

    with torch.no_grad():
        for step in range(CONFIG['steps_per_episode']):
            obs = extract_observations(state, controlled_agents)

            actions = []
            for i, agent in enumerate(agents):
                action = agent.choose_action(obs[i].unsqueeze(0))
                action = action.squeeze(0) if action.dim() > 1 else action
                actions.append(action)

            waymax_action = create_action(actions, controlled_agents, state)
            next_state = sim_env.step(state, waymax_action)

            for i, agent_idx in enumerate(controlled_agents):
                ep_reward += compute_reward(next_state, agent_idx, controlled_agents)
                ep_cost += compute_cost(next_state, agent_idx, controlled_agents)

            state = next_state

    eval_rewards.append(ep_reward / NUM_AGENTS)
    eval_costs.append(ep_cost / NUM_AGENTS)

print("\n" + "=" * 50)
print("üìä EVALUATION RESULTS")
print("=" * 50)
print(f"Avg Reward: {np.mean(eval_rewards):.4f} ¬± {np.std(eval_rewards):.4f}")
print(f"Avg Cost: {np.mean(eval_costs):.4f} ¬± {np.std(eval_costs):.4f}")
print("=" * 50)

# %% [markdown]
# ## 17. Summary
#
# **What we accomplished:**
# 1. ‚úÖ Loaded Waymo data via Waymax (streaming from GCS - NO local data)
# 2. ‚úÖ Used Waymax as simulator with StateDynamics for multi-agent control
# 3. ‚úÖ Trained multi-agent RL policy directly on Waymax observations
# 4. ‚úÖ NO QCNet needed - observations come directly from simulator state
# 5. ‚úÖ Computed rewards: goal reaching + collision avoidance
# 6. ‚úÖ Saved trained models
#
# **All fixes applied:**
# - ‚úÖ v2: Changed from `PlanningAgentEnvironment` to `BaseEnvironment`
# - ‚úÖ v2: Changed from `InvertibleBicycleModel` to `StateDynamics`
# - ‚úÖ v2: Fixed action format to use `TrajectoryUpdate.as_action()`
# - ‚úÖ v3: Fixed batch collection - collect batch_size episodes before update
# - ‚úÖ v5: **Fixed TrafficGamer.py magnet bugs**:
#   - Added `.sum()` to `log_prob()` to convert multi-dim tensor to scalar
#   - Initialize `magnet_signal` before conditional to avoid undefined variable
#   - Wrapped log access in conditional for when magnet is disabled
#
# **Repository:** https://github.com/PhamPhuHoa-23/EvolutionaryTest
